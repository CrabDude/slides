<!DOCTYPE html>
<html>
  <head>
    <title>Testing</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <style type="text/css">
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .remark-inline-code {
        background: #e7e8e2;
        border-radius: 5px;
        padding: 3px;
      }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Testing

Safety starts with you.

---

# Testing

**Package Overview:**

- [Mocha](https://mochajs.org) - **The** node.js test framework
- [Gulp](http://gulpjs.com) - Streaming automated task runner
- [Chai](http://chaijs.com) - For BDD assertions
- [Sinon.js](http://sinonjs.org) - For spies, stubs and mocks
- [SuperTest](https://www.npmjs.com/package/supertest-as-promised) - Extends [SuperAgent](https://github.com/visionmedia/superagent) for conveniently testing HTTP

---

# Terminology

**Integration Test**

Tests the result of systems interacting

*Typically, blackbox testing that is ignorant to internal implementations*

---

# Terminology

**Unit Test**

A test which tests the results of a specific unit of code

*Typically, whitebox testing that isolates internal functionality through the use of spies, stubs and mocks*

---

# Why Test?

**What is unit testing?**

- Write code to test code

---

# Why Test?

**What is unit testing?**

- Write code to test code
- Make sure code functionality works

---

# Why Test?

**What is unit testing?**

- Write code to test code
- Make sure code functionality works
- Granular, single-focus assertions

---

# Why Test?

**What is unit testing?**

- Write code to test code
- Make sure code functionality works
- Granular, single-focus assertions
- Tests results, not internals

---

# Why Test?

**Why unit test?**

- Eliminates the redundancy of testing features

---

# Why Test?

**Why unit test?**

- Eliminates the redundancy of testing features
- Faster than testing functionality manually

---

# Why Test?

**Why unit test?**

- Eliminates the redundancy of testing features
- Faster than testing functionality manually
- Makes code easier to maintain:

---

# Why Test?

**Why unit test?**

- Eliminates the redundancy of testing features
- Faster than testing functionality manually
- Makes code easier to maintain:
  - Confident refactoring

---

# Why Test?

**Why unit test?**

- Eliminates the redundancy of testing features
- Faster than testing functionality manually
- Makes code easier to maintain:
  - Confident refactoring
  - Avoid regressions

---

# Why Test?

**Why unit test?**

- Eliminates the redundancy of testing features
- Faster than testing functionality manually
- Makes code easier to maintain:
  - Confident refactoring
  - Avoid regressions
  - Requiring testability decreases complexity

---

# Why Mocha?

**Features:**

- Runs on node.js & browser
- Unoppinionated on assertion & mocking libraries
- Built-in performance measuring (duration & timeout)
- `async` & `Promise` support
- File watcher support
- Filename pattern matching

---

# Mocha TDD

**[Behavior-driven development](https://en.wikipedia.org/wiki/Behavior-driven_development) (BDD) focuses on *human-readable* tests:**

- Where to start in the process
- What to test and what not to test
- How much to test in one go
- What to call the tests
- How to understand why a test fails

---

# Mocha TDD

**BDD Interface:**

```javascript
// test/test.js
let assert = require('assert')
describe('Test suite', () => {
  describe('feature X', () => {
    it('should have the expected behavior', async () => {
      await process.promise.nextTick()
      assert.equal(true, true)
    })
  })
})
```

- Use `describe` to group tests (nestable)
- Use `it` to describe a test

---

# Mocha TDD

**BDD Interface:**

```javascript
// test/test.js
let assert = require('assert')
describe('Test suite', () => {
  describe('feature X', () => {
    it('should have the expected behavior', async () => {
      await process.promise.nextTick()
      assert.equal(true, true)
    })
  })
})
```

```bash
npm install --save-dev mocha        # Save to "devDependencies" in package.json
npm run mocha test/test.js          # Run a single test
mocha                               # Runs test/*.js
```

---

# Mocha TDD

**Example Output (Basic):**

```bash
$ npm install mocha -g
$ mocha test/test.js









.
```

---

# Mocha TDD

**Example Output (Basic):**

```bash
$ npm install mocha --save-dev
$ npm run mocha test/test.js









.
```

---

# Mocha TDD

**Example Output (Basic):**

```bash

$ npm run mocha test/test.js









.
```

---

# Mocha TDD

**Example Output (Basic):**

```bash

$ npm run mocha  # Run ./test/*.js









.
```

---

# Mocha TDD

**Example Output (Basic):**

```bash

$ npm run mocha  # Run ./test/*.js

> slides-code@1.0.0 test /code
> mocha

  Test suite
    feature X
      âœ“ should have the expected behavior


  1 passing (55ms)
```

---

# Mocha TDD

**Example Output (Advanced):**

![:scale 100%](https://mochajs.org/images/reporter-spec.png)

???

Results are human-readable

---

# Mocha TDD

**Example Output (Failure):**

![:scale 100%](https://mochajs.org/images/reporter-spec-fail.png)

???

Failures are human-readable as well

---

# Mocha TDD

**By convention, use `npm test` to run tests:**

```bash
$ npm test
```

---

# Mocha TDD

**By convention, use `npm test` to run tests:**

```bash
$ npm test
```

Add a `"scripts"` entry to `package.json`:

```javascript
{
  "scripts": {
    // Run tests matching ./test/*.js
    // Use local (devDependencies) or global mocha
    "test": "mocha"
  }
}
```

---

# Mocha TDD

**By convention, use `npm test` to run tests:**

```bash
$ npm test
```

Add a `"scripts"` entry to `package.json`:

```javascript
{
  "scripts": {
    
    // With options
    "test": "mocha --reporter spec altTestDirectory"
  }
}
```

---

# Mocha TDD

**Write tests with ESNext (Babel):**

```bash
$ npm install --save-dev babel-register
$ npm test
```

---

# Mocha TDD

**Write tests with ESNext (Babel):**

```bash
$ npm install --save-dev babel-register
$ npm test
```

```javascript
{
  "scripts": {
    // 1. As a compiler option
    "test": "mocha --compilers js:babel-register"
  }
}
```

---

# Mocha TDD

**Write tests with ESNext (Babel):**

```bash
$ npm install --save-dev babel-register
$ npm test
```

```javascript
{
  "scripts": {
    // 1. As a compiler option
    "test": "mocha --compilers js:babel-register"
    // 2. As a pre-run script (preferred, more configurable)
    "test": "mocha --require customFile_babelhook"
  }
}
```

```javascript
// ./customFile_babelhook.js
require('babel/register')()
```

---

# Mocha: Setup & Teardown

**DRY code with lifecycle hooks:**

- `before` & `after`: run once per group (`describe`)
- `beforeEach` & `afterEach`: run once per test (`it`)

---

# Mocha: Setup & Teardown

**DRY code with lifecycle hooks:**

- `before` & `after`: run once per group (`describe`)
- `beforeEach` & `afterEach`: run once per test (`it`)

```javascript
describe('Test suite', () => {




  describe('feature X', () => {








    it('should have the expected behavior', async () => {


    })
  })
})
```

---

# Mocha: Setup & Teardown

**DRY code with lifecycle hooks:**

- `before` & `after`: run once per group (`describe`)
- `beforeEach` & `afterEach`: run once per test (`it`)

```javascript
describe('Test suite', () => {




  describe('feature X', () => {








    it('should have the expected behavior', async () => {
      let record = await db.findOne({})
      assert.equals(someRecord.id, record.id)
    })
  })
})
```

---

# Mocha: Setup & Teardown

**DRY code with lifecycle hooks:**

- `before` & `after`: run once per group (`describe`)
- `beforeEach` & `afterEach`: run once per test (`it`)

```javascript
describe('Test suite', () => {
  before(async () => {
    await db.connect()
  })

  describe('feature X', () => {
    
      
    

    
    
    

    it('should have the expected behavior', async () => {
      let record = await db.findOne({})
      assert.equals(someRecord.id, record.id)
    })
  })
})
```

---

# Mocha: Setup & Teardown

**DRY code with lifecycle hooks:**

- `before` & `after`: run once per group (`describe`)
- `beforeEach` & `afterEach`: run once per test (`it`)

```javascript
describe('Test suite', () => {
  before(async () => {
    await db.connect()
  })

  describe('feature X', () => {
    beforeEach(async () => {
      await db.save(someRecord)
    })





    it('should have the expected behavior', async () => {
      let record = await db.findOne({})
      assert.equals(someRecord.id, record.id)
    })
  })
})
```

---

# Mocha: Setup & Teardown

**DRY code with lifecycle hooks:**

- `before` & `after`: run once per group (`describe`)
- `beforeEach` & `afterEach`: run once per test (`it`)

```javascript
describe('Test suite', () => {
  before(async () => {
    await db.connect()
  })

  describe('feature X', () => {
    beforeEach(async () => {
      await db.save(someRecord)
    })

    afterEach(async () => {
      await db.clear()
    })

    it('should have the expected behavior', async () => {
      let record = await db.findOne({})
      assert.equals(someRecord.id, record.id)
    })
  })
})
```

---

# Mocha: Test Options

**1. Pending tests (aka "TODO"):**

```javascript
describe('Feature X', () => {
  // No callback 
  it('should do something not yet supported')
})
```

---

# Mocha: Test Options

**1. Pending tests (aka "TODO"):**

```javascript
describe('Feature X', () => {
  // No callback 
  it('should do something not yet supported')
})
```

**2. Skipped tests:**

```javascript
describe('Feature X', () => {
  it.skip('should ignore this test that\'s currently broken', ...)
  it('should still pass this test', ...)
})
```

---

# Mocha: Test Options

**1. Pending tests (aka "TODO"):**

```javascript
describe('Feature X', () => {
  // No callback 
  it('should do something not yet supported')
})
```

**2. Skipped tests:**

```javascript
describe('Feature X', () => {
  it.skip('should ignore this test that\'s currently broken', ...)
  it('should still pass this test', ...)
})
```

**3. Exclusive tests:**

```javascript
describe('Feature X', () => {
  it.only('should pass this test because it\'s currently being worked on', ...)
  it('should ignore this test at the moment', ...)
})
```

---

# Mocha: Test Options

**1. Pending tests (aka "TODO"):**

```javascript
describe('Feature X', () => {
  // No callback 
  it('should do something not yet supported')
})
```

**2. Skipped tests:**

```javascript
describe('Feature X', () => {
  it.skip('should ignore this test that\'s currently broken', ...)
  it('should still pass this test', ...)
})
```

**3. Exclusive tests:**

```javascript
describe('Feature X', () => {
  it.only('should pass this test because it\'s currently being worked on', ...)
  it('should ignore this test at the moment', ...)
})
```

**Note:** All of the above can be applied to `describe` as well.
---

# Mocha: CLI

**Many, many options. Here are a few:**

- `$ mocha test/test.js`
- `$ mocha --watch test/test.js`
- `$ mocha --grep="Test suite" test/test.js` *
- Etc... `$ mocha --check-leaks --timeout 5000`

\* Only runs tests with "Test suite" in descriptions

---

# Mocha: Reporters

**Reporters determine how results are displayed:**

```bash
$ mocha --reporters

    dot - dot matrix
    doc - html documentation
    spec - hierarchical spec list
    json - single json object
    progress - progress bar
    list - spec-style listing
    tap - test-anything-protocol
    landing - unicode landing strip
    xunit - xunit reporter
    html-cov - HTML test coverage
    json-cov - JSON test coverage
    min - minimal reporter (great with --watch)
    json-stream - newline delimited json events
    markdown - markdown documentation (github flavour)
    nyan - nyan cat!
```

---

# Mocha: Reporters

**Spec:**

The default reporter, a hierarchical nested view.

![:scale 100%](https://mochajs.org/images/reporter-spec.png)

---

# Mocha: Reporters

**Dot Matrix:**

Dots representing test cases, failures highlight in **red**, pending in **blue**, slow as **yellow**.

![:scale 100%](https://mochajs.org/images/reporter-dot.png)

---

# Mocha: Reporters

**List:**

A simple specifications list as test cases pass or fail, outputting the failure details at the bottom of the output.

![:scale 100%](https://mochajs.org/images/reporter-list.png)

---

# Mocha: Reporters

**Nyan:**

Nyan, nyan, nyan, nyan, nyan, nyan, nyan, nyan, nyan, nyan, nyan, nyan, nyan, nyan, nyan, nyan, nyan, nyan, nyan, nyan, nyan, nyan, nyan, nyan, nyan, nyan, nyan, nyan, nyan...

![:scale 100%](https://mochajs.org/images/reporter-nyan.png)

---

# Mocha: Reporters

**JSON:**

A single large JSON object for programmatic consumption (e.g., tooling, instrumentation).

![:scale 100%](https://mochajs.org/images/reporter-json.png)

---

# Mocha: Examples

**Test Suites from projects using Mocha:**

- `express`: https://github.com/strongloop/express/tree/master/test
- `connect`: https://github.com/senchalabs/connect/tree/master/test
- `superagent`: https://github.com/visionmedia/superagent/tree/master/test/node
- `mocha`: https://github.com/visionmedia/mocha/tree/master/test
- Websocket.io (browser): https://github.com/LearnBoost/websocket.io/tree/master/test

---

# Gulp: Streaming Task Runner

**[`gulp`](https://www.npmjs.com/package/gulp) & [`gulp-mocha`](https://www.npmjs.com/package/gulp-mocha):**

```bash
$ npm install gulp-cli  --save-dev      # The task runner CLI
$ npm install gulp --save-dev           # The task runner dependency
$ npm install gulp-mocha --save-dev     # Run mocha from gulp
```

---

# Gulp: Streaming Task Runner

**Basic example:**

```javascript
// ./gulpfile.js
let gulp = require('gulp')
let mocha = require('gulp-mocha')

gulp.task('default', () => {


})


.
```

---

# Gulp: Streaming Task Runner

**Basic example:**

```javascript
// ./gulpfile.js
let gulp = require('gulp')
let mocha = require('gulp-mocha')

gulp.task('default', () => {
  return gulp.src('test.js', {reporter: 'spec'})
    .pipe(mocha())
})


.
```

---

# Gulp: Streaming Task Runner

**Basic example:**

```javascript
// ./gulpfile.js
let gulp = require('gulp')
let mocha = require('gulp-mocha')

gulp.task('default', () => {
  return gulp.src('test.js', {reporter: 'spec'})
    .pipe(mocha())
})


.
```

```bash
$ npm run gulp                          # Run all tasks
```

---

# Gulp: Streaming Task Runner

**Basic example:**

```javascript
// ./gulpfile.js
let gulp = require('gulp')
let mocha = require('gulp-mocha')

gulp.task('tests', () => {
  return gulp.src('test.js', {reporter: 'spec'})
    .pipe(mocha())
})

// The default task (called when you run `gulp` from cli) 
gulp.task('default', ['tests'])
```

```bash
$ npm run gulp test                      # Run specific tasks
```

---

# Gulp: Streaming Task Runner

**Use `gulp.watch` to rerun tasks when files change:**

```javascript
// ./gulpfile.js
let gulp = require('gulp')
let mocha = require('gulp-mocha')

gulp.task('watch', () => {
  // Assume we declared these tasks
  gulp.watch('test/**/*', ['tests'])
  gulp.watch('src/**/*', ['babel-build'])
})

.
```

```bash
$ npm run gulp watch                     # Rerun tasks on file change
```

---

# Why Chai.js?

**BDD Assertion Library:**

---

# Why Chai.js?

**BDD Assertion Library:**

- Human-readable assertions (chaining):
  ```javascript
  foo.should.be.a('string')
  ```

---

# Why Chai.js?

**BDD Assertion Library:**

- Human-readable assertions (chaining):
  ```javascript
  foo.should.be.a('string')
  ```
- node.js and browser supported

---

# Why Chai.js?

**BDD Assertion Library:**

- Human-readable assertions (chaining):
  ```javascript
  foo.should.be.a('string')
  ```
- node.js and browser supported
- Supports common assertion styles:
  ```javascript
  // should
  foo.should.be.a('string')

  // expect
  expect(foo).to.be.a('string')

  // assert
  assert.typeOf(foo, 'string')
  ```

---

# Chai.js: Example

```bash
$ npm install chai --save-dev
```

---

# Chai.js: Example

```bash
$ npm install chai --save-dev
```

**BDD with `should`:**

```javascript
// Just some features
x.should.be.empty
x.should.deep.equal(someObject)
x.should.have.property('y', someValue)
x.should.throw('error message')

// Empty check
should.exist(x)

// Negation
x.should.not.be.an('object')

// Chaining
x.should.not.be.an('object')
  .and.have.property('foo').to.deep.equal(someObject.foo)
```

---

# Why Sinon.js?

**Standalone test spies, stubs and mocks:**

- Spy:
  - A function that records the arguments, return value, the value of `this` and exception thrown (if any) for all its calls
  - Can be an anonymous function or wrap an existing function
  - Especially useful for wrapping callbacks
- Stub:
  - Spies with pre-programmed **behavior**
  - Extends the Spy API with methods to alter the stubâ€™s behavior.
  - Spies are passive observers, stubs are for forcing function behavior
  - **Does not** wrap an existing function
  - Especially useful for avoiding IO in unit tests
- Mock:
  - Stubs with pre-programmed **expectations**
  - Primarily used to state assertions up front

---

# Sinon.js

**Spies:**

Passive function observers, userful for later assertions.

```javascript
let sinon = require('sinon')
require('sinon-chai')

describe('Calling events with `emit`', () => {
  it('should call all the functions bound in the queue', () => {









  })
})
```

---

# Sinon.js

**Spies:**

Passive function observers, userful for later assertions.

```javascript
let sinon = require('sinon')
require('sinon-chai')

describe('Calling events with `emit`', () => {
  it('should call all the functions bound in the queue', () => {
    let e = new EventEmitter
    let fn = sinon.spy()



    e.on('test', fn)
    e.emit('test', 1, 2)

    fn.should.be.calledWith(1, 2)
  })
})
```

---

# Sinon.js

**Stubs:**

Stubs are spies with pre-programmed behavior.

```javascript
let sinon = require('sinon')
require('sinon-chai')

describe('Callback`', () => {
  it('should return the expected values', () => {









  })
})
```

---

# Sinon.js

**Stubs:**

Stubs are spies with pre-programmed behavior.

```javascript
let sinon = require('sinon')
require('sinon-chai')

describe('Callback`', () => {
  it('should return the expected values', () => {
    let callback = sinon.stub()
    callback.withArgs(42).returns(1)
    callback.withArgs(1).throws('TypeError')



    callback().should.be.empty
    callback(42).should.equal(1)
    callback().should.throw('TypeError')
  })
})
```

---

# Sinon.js

**Mocks:**

Mocks are stubs with pre-programmed (up-front) expectations.

```javascript
let sinon = require('sinon')
require('sinon-chai')

describe('Callback`', () => {
  it('should return the expected values', () => {









  })
})
```

---

# Sinon.js

**Mocks:**

Mocks are stubs with pre-programmed (up-front) expectations.

```javascript
let sinon = require('sinon')
require('sinon-chai')

describe('Callback`', () => {
  it('should return the expected values', () => {
    let e = new EventEmitter
    let mock = sinon.mock({handler: () => {}})
    mock.expects('handler').once().throws()

    e.on('test', mock.handler)
    e.emit('test', 1, 2)
    e.emit('someOtherEvent')

    mock.verify()
  })
})
```

---

# SuperTest: HTTP Mocking

**Convenient HTTP client request mocking built on SuperAgent:**

```bash
$ npm install supertest --save-dev
```

Simple Example:

```javascript
let request = require('supertest-as-promised')

async function foo() {
    await request(app) // Pass the http.Server instance
      .get('/user')
      .set('Accept', 'application/json')
      .expect('Content-Type', /json/)
      .expect(200)
}
```

---

# SuperTest: HTTP Mocking

**Convenient HTTP client request mocking built on SuperAgent:**

```bash
$ npm install supertest-as-promised --save-dev
```

Advanced Example:

```javascript
let request = require('supertest-as-promised')
describe('GET /user', () => {
  it('should work', async () => {
    await request(app)
      .get('/user')
      .set('Accept', 'application/json')
      .expect((res) => {
        res.body.id = 'some fixed id'
        res.body.name = res.body.name.toUpperCase()
      })
      .expect(200, {
        id: 'some fixed id',
        name: 'TOBI'
      })
  })
})
```

---

class: center, middle

# Questions?

    </textarea>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.8.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.8.0/highlight.min.js"></script>
    <script src="https://gnab.github.io/remark/downloads/remark-latest.min.js">
    </script>
    <script>
      remark.highlighter.engine = hljs
      remark.macros.scale = function (percentage) {
        var url = this
        return '<img src="' + url + '" style="width: ' + percentage + '" />'
      }
      var slideshow = remark.create({
        highlightStyle: 'tomorrow-night-bright'
      })
    </script>
  </body>
</html>
